base_model: Qwen/Qwen2-7B-Instruct
model_type: AutoModelForCausalLM
torch_dtype: float16
bf16: false
fp16: true 
trust_remote_code: true

datasets:
  - path: data/tsun_sft.jsonl
    type: alpaca


attn_implementation: sdpa
dataset_prepared_path: .cache/ax_leo
output_dir: ckpts/tsun-qwen2-7b-qlora
save_safetensors: true
save_steps: 200
logging_steps: 20
eval_steps: 200
evaluation_strategy: steps
do_eval: true
val_set_size: 0.1

sequence_len: 2048
sample_packing: false

adapter: lora
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
target_modules: [q_proj,k_proj,v_proj,o_proj,up_proj,down_proj,gate_proj]

gradient_checkpointing: true
micro_batch_size: 1               
per_device_train_batch_size: 2
per_device_eval_batch_size: 2
batch_size: 16
optim: adamw_torch
lr_scheduler: cosine
learning_rate: 1.0e-4
weight_decay: 0.01
max_grad_norm: 0.3
warmup_ratio: 0.03
num_train_epochs: 2
report_to: none
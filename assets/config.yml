# ===== 基本 =====
model_name_or_path: C:\Users\User\Hugging face\hf-models\qwen7b   # 換成你要的模型或本地路徑
output_dir: D:\Qwen_SFT\qwen7b_7b_qlora_sft
stage: sft
do_train: true
do_eval: true                                           # 有驗證集再開
seed: 42

# ===== 資料 =====
dataset: my_dataset
eval_dataset: my_validation_set
dataset_dir: data\my_dataset

# ===== 訓練超參 =====
per_device_train_batch_size: 4
gradient_accumulation_steps: 8
learning_rate: 1e-4
num_train_epochs: 4
logging_steps: 10
save_steps: 200
warmup_ratio: 0.03
lr_scheduler_type: cosine
gradient_checkpointing: true
resume_from_checkpoint: false

# ===== 精度 / 記憶體 =====
bf16: true
fp16: false
quantization_bit: 4                                       # QLoRA
optim: paged_adamw_32bit
max_grad_norm: 0.3

# ===== LoRA =====
finetuning_type: lora
lora_rank: 64
lora_alpha: 128
lora_dropout: 0.05
# 保守一點全面覆蓋注意力 + MLP（不同模型層名略有差異）
lora_target: [q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj]

# ===== Windows/新卡避雷（先求能跑通，再加速）=====

# bitsandbytes 在 Windows/新架構偶爾有坑，此配置以 QLoRA + paged_adamw_32bit 為主

base_model: Qwen/Qwen2-7B
load_in_4bit: true  #4Bit的精度
bnb_4bit_compute_dtype: float16
gradient_checkpointing: true #梯度檢查點
lora_r: 64
lora_alpha: 16
lora_dropout: 0.05

#transformer用的資料頭?
target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj

dataset: data/tsun_mentor_sft.jsonl #設定檔的位置
dataset_format: instruct
output_dir: out/tsun-mentor-qwen7b-lora
sequence_len: 2048
micro_batch_size: 1
gradient_accumulation_steps: 8
epochs: 2
lr: 1.5e-4
warmup_ratio: 0.03
optimizer: adamw_torch
save_steps: 100
logging_steps: 10